---
title: "Regression Introduction"
author: "brouwern@gmail.com"
date: "January 30, 2017"
output:
  html_document: default
  word_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 9
    reference_docx: rmarkdown_template.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, strip.white = FALSE, cache = TRUE)
```




# What does regression do?

```{r, echo = F}
#load data
milk2 <- read.csv(file = "Skibiel_clean_milk_focal_column.csv")
milk3 <- read.csv(file = "Skibiel_clean_milk_focal_genera.csv")
```


```{r, echo=FALSE}
par(mfrow = c(1,1))
source("./scripts_Lecture4/sx_reg_flowchart1_inf_predict_model.R")
```

* INFERENCE: is body size significantly related to % milk fat?
* PREDICTION: ie, given the body size of an extinct animal, what do we ...
    + predict its % milk fat to be?
    + how certain could we be in this prediction?
* MODEL SELECTION: what's the best predictor of milk fat?
    + body size?
    + litter size?
    + lactation duration?

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

# Steps in regression

```{r, echo=FALSE}
source("./scripts_Lecture4/sx_reg_flowchart2_fit_diag_inf.R")
```

* Standard approach
    + Model fitting: least squares
    + Inference: Frequentist
    + (In particular: "NHST" w/alpha = 0.05)
    + (="Null hypothesis significance testing")
* More advanced models: 
    + Model fitting: "GLS"
    + "generalized least square"
    + gls() function, nlme package
    + can relax standard regression assumptions
    + This tool isn't used by bio/ecologists much, but should be!
* Advanced models: Likelihood
    + For "GLMs": generalized linear models
    + i.e. logistic regression
    + repeated measures, mixed effects, random effects often fit with likelihood methods
* Very Complex models: Bayes w/ MCMC
    + very complex/hiearchical or multilevel models
    + MCMC: markov chain monte carlo


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

## Aide: what do I mean by "multilevel" or "hiearchical"

**What is it?**

* Many names for same / similar issue
    + repeated measures
    + blocking
    + random effects / mixed effects models
    + multilevel models
    + hiearchical models
* Measurements on same thing multiple times or on similar experimental units are likely to be more similar to each other than random
* Violates the assumption of independence in regression/ANOVA

<br><br><br><br><br><br><br>

**Multilevel Examples:**

* Education studies
    + students in class rooms in schools in districts in states
    + (multiple students per class, multiples classes per school, multiple schools per district)
* Health care studies
    + patients on hospital floors in hospitals in cities
* Animal studies
    + repeated measures on mice in cages
    + (multiple measurements per mice, multiple mice per cage)

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


# Regression models in R using lm()

## Basic R regression


* "lm()" = "linear model""
* regression, t.test, ANOVA, ANCOVA are all linear models
* all are fit with lm() function
* (Welch's correct only done with t.test; similar effect with gls())
* "gls()" = generalized least squares
* "glm()" = generalized linear model


```{r}
lm.mass <- lm(fat.percent ~ log(mass.female),
             data = milk3)
```

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


## Model summary

* Look at model w/ summary()
* (we can get just the coefficients/slopes with coef() )
* (anoter useful function is arm::display; has shorter output)
* (recall that you can save these summary outputs and access them as lsits with a $)

```{r, echo = -1}
options(digits=3)
summary(lm.mass)
```

We'll unpack what each of these is

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

### Plot model w/ sumamry output
 
```{r, echo = F}
source("./scripts_Lecture4/sx_mtext_plot_coefs_in_margin.R")
mar.default <- c(c(5, 4, 4, 1) + 0.1)
par(mfrow = c(1,1), mar = c(5, 4, 4, 2) + 0.1)

plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
abline(lm.mass, col =3, lwd  =2)

m.coefs(lm.mass)

```

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>



### Plot model w/ equations

```{r, echo = F}
par(mfrow = c(1,1))
source("./scripts_Lecture4/sx_mtext_plot_equations_in_margin.R")
plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
abline(lm.mass, col =3, lwd  =2)
m.equations(lm.mass)

```

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>



```{r, echo = F}
### Plot model w/ wider x limits
#function to make plot
make.plot <- function(){
  plot(fat.percent ~ log(mass.female),
       data = milk3,
       ylab = "Fat content of milk",
       pch = 18,
       cex =2,
       xlim = c(0,12))
  abline(lm.mass, col =3, lwd  =2)
}

```

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


### Add location of intercept

```{r, echo=F}
par(mfrow = c(1,1), mar = c(4.1,4.1,4,1))
make.plot()
y. <- coef(lm.mass)[1]
abline(v = 0, col = 2,lwd = 2)
arrows(x0 = 1,x1=0,
       col = 2,lwd = 2,
       y0 = y.)
lab. <- round(coef(lm.mass)[1],2)
text(x = 1.71,y = y.,
     label = lab.)
m.coefs(lm.mass)
```

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


# Coefficient plot
## What is this?

* arm::coefplot()
* NOTE: by convention, axes are flipped!

```{r, echo = F}
library(coefplot)
arm::coefplot(lm.mass,intercept = T,
              mar=c(1,9,5.1,2),
              cex.pts = 2,
              col.pts = 2:1,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
#mtext(text = mod,side = 1,adj= 1, line = 0)


```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

# Coefficient plot
## What is this?

* arm::coefplot()
* NOTE: by convention, axes are flipped!

```{r, echo = F}
library(coefplot)
arm::coefplot(lm.mass,intercept = T,
              mar=c(1,9,5.1,2),
              cex.pts = 2,
              col.pts = 2:1,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
#mtext(text = mod,side = 1,adj= 1, line = 0)

text(x  = 1, y = 2, label = "Slope of line",pos = 4, cex = 2)

```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

## Compare coefficient plot to raw data

```{r}
par(mfrow = c(1,2), mar = c(5,2,7,0.25))

arm::coefplot(lm.mass,intercept = T,
              mar=c(5,6,8,1),
              cex.pts = 2,
              col.pts = 1:2,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
make.plot()

y. <- coef(lm.mass)[1]
abline(v = 0, col = 2,lwd = 2)
arrows(x0 = 1,x1=0,
       col = 2,lwd = 2,
       y0 = y.)
lab. <- round(coef(lm.mass)[1],2)
text(x = 1.71,y = y.,
     label = lab.)

```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


# Linear regression as model fitting

## Fit 2 models

### Null model (Ho): flat line

* Null hypothesis: ????
* Null model: "fat.percent ~ 1"
* "~1" means fit model 
    + w/ slope = 0 
    + intercept = mean of response variable

```{r}
lm.null <- lm(fat.percent ~ 1,
              data = milk3)

```


### Alternative model: fat.percent ~ log(mass.female)

```{r}

lm.mass <- lm(fat.percent ~ log(mass.female),
              data = milk3)


```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>




## Plot both models

```{r, echo = F}
par(mfrow = c(1,2),mar = c(4.1,4.1,2,1))
plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
mtext("Ho: Null model",cex = 1.5)
abline(lm.null, col =4, lwd  =3, lty = 3)
legend("topright",
       col = c(4),
       lty = 3,
       lwd = 4,
       legend = c("y ~ 0*log(mass) + int.null"))

plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "",
     pch = 18,
     cex =2)
mtext("Ha: Alt model",cex = 1.5)
abline(lm.mass, col =3, lwd  =3)

legend("topright",
       col = c(3,4),
       lty = 1:2,
       lwd = 3,
       legend = c("y ~ slope*log(mass) + int.alt"))
```



<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


## What is the intercep of null model?

The mean fat value is 8.6
```{r}
summary(milk3[,"fat.percent"])
```


```{r, echo = F}
par(mfrow = c(1,2))
plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "")
abline(lm.null, col =3, lwd  =3)
mtext("Ho: Null model",cex = 1.5)
legend("topright",
       legend = c("y ~ 0 * log(mass) + 8.6"))


plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "")
abline(lm.mass, col =4, lwd  =3)
mtext("Ha: Alt model",cex = 1.5)
legend("topright",
       legend = c("y ~ -1.75 * log(mass) + 20.5"))
```




<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>



# Test model: significance test

## Nested Models
### Ha: y ~ -1.75*log(mass) + 20.5
### Ho: y ~     0*log(mass) +  8.6


* Ha has TWO parameters (aka coefficients, betas)
    + Intercept = `r round(coef(lm.mass)[1],3)`
    + Slope     = -1.75
* Ho has ONE parameter
    + Intercept = 8.6
    + (Slope = 0, so it doesn't count)
* Hypothesis can be formulated of 2 "nested models"
    + Applies to t-test, ANOVA, ANCOVA
    + A major difference between Hypothesis testing w/p-values and IT-AIC is that AIC doesn't require models to be nested!

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


## Hypothesis test of nested models in R

* anova() 
* carries out an F test for linear models
* "likelihood ratio test" for GLMs/GLMMs
    + GLMM = generalized linear mixed models
    + Models w/random effects, blocking, repeated measures etc
    
```{r}
#NOTE: order w/in anova() doesn't matter
anova(lm.null, 
      lm.mass)
```

* Output of anova() is similar to content of summary()
* I like to emphasize the testing of nested models
* GLMs, GLMMs often can only be tested with anova()


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


# Examine model: IT-AIC

## IT-AIC

* "Information Theory- Akaike's Information Criteria"
* "Information Theory" is a general approach to investiting models
* AIC is a particular information theory
    + Others: BIC, WAIC, FIC, DIC (Bayesian), ...
    + seems like every statistican has one now
* AICc: correction for small samples size
    + Should always use AICc
* qAIC: corretion for "overdispersion"
    + For GLMs (ie. logistic regression)

## AIC in base R

* Note: no AICc command in base R!
* Other packages can do AICc and other ICs
* DF = number of parameters
```{r}
AIC(lm.null, 
    lm.mass)
```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


## AIC table

* Lower is "better"
* Absoluate values have NO MEANSING
* AICs are only interpretable relative to other AIC
* focus: "delta AIC" (dAIC)
* AIC goes down if a model fits the data better
* BUT - every additional parameter has a "penalty"

```{r}
library(bbmle)

AICtab(lm.null,
       lm.mass,
       base = T)
```



## AICc 

* AICc is a correction for small sample size
* Doesn't make much difference here

```{r}
library(bbmle)

ICtab(lm.null,
      lm.mass,
      type = c("AICc"),
      base = T)
```


<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

# What about R^2?

### Hypothesis testing:
* p-values: tell if you if a predictor is "significantly" associated w/ a respons
* Does not tell you the EFFECT SIZE is large
    + p can be very very small
    + but slope also very small
    
### IC-AIC
* AIC tells you if one model fits the data better than another
* AIC doesn't tell you if either model data very well

### R^2
* Tells you how much variation in the data is explained by model
* whether you use p-values or AIC, you should report R^2
    



<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>


## Null vs Alternative model

* Slope of Alt models is highly significant
* But R2 isn't huge
* Lots of scatter around line
* Other predictors might improve fit
```{r, echo = F}

par(mfrow = c(1,2),
    mar = c(4,4,2,1))
R2.null <- paste("R2 =",summary(lm.null)$r.squared)

plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "Null model (Ho)")
abline(lm.null, col =3, lwd  =3)
legend("topright",
       legend = R2.null)

R2.Ha <- paste("R2 =",
               round(summary(lm.mass)$r.squared,2)
               )
plot(fat.percent ~ log(mass.female),
     data = milk3,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "Alt. model (Ha)")
abline(lm.mass, col =4, lwd  =3)
legend("topright",
       legend = R2.Ha)
```
